/**

@mainpage 15=410 Project 2

@author Jonathan Ong (jonathao)
@author Evan Palmer (esp)



Concurrecy
==========

Mutexes
=======

Our mutexes are fairly standard. They use an atomic exchange instruction to
aquire the lock, and threads which fail to aquire the lock yield because the
kernel is single threaded.

Our mutexes have a few optimizations over a simple implementaton

while(atomic_xchg(mutex->lock, LOCKED) == LOCKED){
    yield(-1);
}

When a thread aquires the lock on a mutex, it sets itself as the owner of that
mutex. When the owner is set, other threads which fail to get to the lock will
yield to the owner instead of to an arbitrary thread. This helps in the case
where many threads are waiting on a single mutex, since once the owner is set
the threads will yield to the thread with the mutex.

Additioanly we keep an atomic counter of the number of threads waiting on the
mutex. When threads attempt to lock the mutex, they first optimiztically try
to grab the lock, and if that fails, they increment the waiting counter and
enter the yield and try to lock loop.

When a thread releases a mutex, it checks to see if there are currently any
threads waiting. If there are threads waiting on the mutex, the unlocking thread
yields to an arbitrary thread. This prevents a single thread from hogging the
lock in a case where there is code like.

for(;;) {
    mutex_lock(mutex);
    i++;
    mutex_unlock(mutex);
}

We considered implementing locks with an even stronger fairness guarantee
using fetch and add to produce tickts, but it seemed like under heavy contention
these locks would require many calls to yield, since aquiring thread would have
to yield until the thread with the correct ticket got scheudled. Maybe this
could be improved by adding a lock free datastructure which recorded who had
what ticket, but that seemed unnecesarily complex.

Our locks have the nice property that in the common case, they are very simple.
If a lock is not contended, aquiring the lock is little more than an exchange
and releasing the lock does not include yeilding. It is only under contention
that we use the atomic counters and yield tricks.

Condition Varaibles
===================

Our condition variable implementation is not very exciting. In cond_wait
we needed to atomically release the conditon variable mutex and deschedule
the thread. This operation only needs to be atomic with respect to cond_signal,
so we implement the guarantee there.

In cond_signal, when waking up a thread, we keep in mind that the thread might
not yet have been descheduled. If the schedule system call returns an error,
we yield to the thread we are attempting to schedule, allowing it to deschedule
itself before we attempt again. So, we only conclude that a thread has been
awoken once the schedule system call has been run without an error.

Semaphores
==========
Our semaphores were implemented using mutexes and condition variables. Our
implementation is straightforward and gets its guarantees for fairness from
our implementations of mutexes and condition variables.


I have nothing to say

Reader Writer Locks
===================
Our reader/writer locks were implemented using condition variables and mutexes.

We prevent the starvation of readers by allowing readers to acquire the lock as
long as there are no writers waiting, and by ensuring that readers which join
the queue ahead of writers will get to go in that order. Whenever one reader
acquires the lock, all readers waiting in line at that point in time will also
acquire the lock to maximize efficiency. If there are writers waiting, new
readers will have to wait for those writers to finish before proceeding.
Readers are guaranteed a finite number of writers to wait for (at most all the
writers in the queue at the point in time the reader joins).

We prevent the starvation of writers by ensuring that whenever a writer joins
the waiting queue and the lock is in read mode, all future readers attempting
to acquire the lock will wait instead of proceeding. Thus the waiting writers
have a finite number of readers to wait on (all readers in the queue at the
point in time when the writer joins) before they are guaranteed to acquire
the lock.

Thread Library
==============

Thread init
===========

Guarantees for thread stack size and stuff

tid is system tid and why
-------------------------

We chose our thread ids to match the system thread id. There are cases like
an M:N system thread to thread implementation, but it works in our case.
Additionally, using this implmentation allows us for very simple and efficient
implementations of thr_yield, and greatly simplifies calling schedule and
deschedule in our condition variable code.

Fine grained locks
------------------

Frame Allocator
===============

Reusing Frames
--------------

Thread Create
=============

Storing in registers for create
-------------------------------

Our thread create is written in assembly since once we are in the child thread
we don't want to use the stack until we have set up the stack pointer so we
are not on our parent's stack.

Making sure tcb is created when function returns
------------------------------------------------

Thread create is responsible for adding the entry for the created thread to the
tcb. It is important that when the parent returns, and when the child starts
running this tcb entry exists. To ensure this, the first thread which is run
will create the tcb entry, and then wait on a condition variable for the second
thread to acknowledge that the entry has been created.

Simply checking if the tcb entry exists and creating it if not in both threads
is not sufficient to prevent a race. If the child thread exits and is joined
before the main thread gets to run, this would result in an extra tcb entry
being added for the child.

Thread Exit and Join
====================

Freeing the frame without weird race
-------------------------------------

Making sure only one person is joining
--------------------------------------

Waiting and signaling with exit
------------------------------

Thread get id
=============

As part of thread create, we store the created thread's id at the top of its
thread stack. This allows us to look up the thread id quickly in normal
circumstances. To do this, we look at the current value of esp, and determine
which thread stack it is on, and then return the id at the top of that stack.

In cases where esp does not point to a region associated with a thread stack,
we defer to the gettid system call (this could be the case if a thread was
in an exception handler perhaps). It is reasonable to assume that threads
will not execute on other thread's stacks.


Thread exit and exit override
-----------------------------

We wrote an alternate version of exit which get's linked instead of the
provided exit function. This version of exit calls thread exit if the thread
library has been initalized, and does the normal set_status and vanish if it
has not.

The correctness of this solution does depend on the link order of the program.
With the current Makefile, it works, but it's not as clean as we might like.
A nicer solution would be to user the --wrap exit flag in gcc.


Autostack Handler
=================

We did it! Grow the required amount, reregister handler

*/
